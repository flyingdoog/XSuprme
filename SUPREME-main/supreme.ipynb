{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c69786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPREME is setting up!\n"
     ]
    }
   ],
   "source": [
    "# Options\n",
    "addRawFeat = True\n",
    "base_path = ''\n",
    "feature_networks_integration = ['clinical', 'cna', 'exp','coe','met','mut'] # datatypes to concatanate node features from\n",
    "node_networks = ['clinical', 'cna', 'exp','coe','met','mut'] # datatypes to use networks from\n",
    "int_method = 'MLP' # Machine Learning method to integrate node embeddings: 'MLP' or 'XGBoost' or 'RF' or 'SVM'\n",
    "\n",
    "# optimize for hyperparameter tuning\n",
    "learning_rates = [0.01, 0.001, 0.0001] # learning rates to tune for GCN\n",
    "hid_sizes = [32, 64, 128, 256] # hidden sizes to tune for GCN\n",
    "xtimes = 50 #number of times Machine Learning algorithm will be tuned for each combination\n",
    "xtimes2 = 3 # number of times each evaluation metric will be repeated (for standard deviation of evaluation metrics)\n",
    "\n",
    "# optimize for optional feature selection of node features\n",
    "feature_selection_per_network = [False, False, False,False, False, False]\n",
    "top_features_per_network = [50, 50, 50,50,50,50]\n",
    "optional_feat_selection = False\n",
    "boruta_runs = 100\n",
    "boruta_top_features = 50\n",
    "\n",
    "# fixed\n",
    "max_epochs = 500\n",
    "min_epochs = 200\n",
    "patience = 30\n",
    "\n",
    "# fixed to get the same results from the tool each time\n",
    "random_state = 404\n",
    "\n",
    "# SUPREME run\n",
    "print('SUPREME is setting up!')\n",
    "from lib import module\n",
    "import time\n",
    "import os, itertools\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import errno\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "if ((True in feature_selection_per_network) or (optional_feat_selection == True)):\n",
    "    import rpy2\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects.packages import importr\n",
    "    utils = importr('utils')\n",
    "    rFerns = importr('rFerns')\n",
    "    Boruta = importr('Boruta')\n",
    "    pracma = importr('pracma')\n",
    "    dplyr = importr('dplyr')\n",
    "    import re\n",
    "\n",
    "dataset_name = 'full_data'\n",
    "\n",
    "path = base_path + \"data/\" + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf3f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random split\n",
      "SUPREME is running..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [00:02<00:03, 90.45it/s] \n",
      " 40%|████      | 200/500 [00:01<00:01, 162.09it/s]\n",
      " 49%|████▉     | 247/500 [00:01<00:01, 158.19it/s]\n",
      " 40%|████      | 200/500 [00:01<00:02, 145.31it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 154.05it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 163.36it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 156.29it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 153.59it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 151.50it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 162.79it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 168.84it/s]\n",
      " 40%|████      | 200/500 [00:01<00:02, 147.71it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 167.68it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 159.71it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 155.45it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 150.53it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 155.55it/s]\n",
      " 40%|████      | 200/500 [00:01<00:01, 161.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 28.2 seconds for node embedding generation (12 trials for 6 seperate GCNs).\n",
      "SUPREME is integrating the embeddings..\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, emb1 = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return emb1\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out, emb2 = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        loss = criterion(out[data.valid_mask], data.y[data.valid_mask])        \n",
    "    return loss, emb2\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data_path_node =  base_path + 'data/' + dataset_name +'/'\n",
    "run_name = 'SUPREME_'+  dataset_name + '_results'\n",
    "save_path = base_path + run_name + '/'\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n",
    "\n",
    "file = base_path + 'data/' + dataset_name +'/labels.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "if os.path.exists(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        train_valid_idx, test_idx = pickle.load(f)\n",
    "    print('use pre-defined split')\n",
    "else:\n",
    "    train_valid_idx, test_idx= train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels)\n",
    "    print('use random split')\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('SUPREME is running..')\n",
    "# Node feature generation - Concatenating node features from all the input datatypes            \n",
    "for netw in node_networks:\n",
    "    file = base_path + 'data/' + dataset_name +'/'+ netw +'.pkl'\n",
    "    with open(file, 'rb') as f:\n",
    "        feat = pickle.load(f)\n",
    "        if feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] < feat.values.shape[1]:     \n",
    "            feat_flat = [item for sublist in feat.values.tolist() for item in sublist]\n",
    "            feat_temp = robjects.FloatVector(feat_flat)\n",
    "            robjects.globalenv['feat_matrix'] = robjects.r('matrix')(feat_temp)\n",
    "            robjects.globalenv['feat_x'] = robjects.IntVector(feat.shape)\n",
    "            robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "            robjects.globalenv['top'] = top_features_per_network[node_networks.index(netw)]\n",
    "            robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "            robjects.r('''\n",
    "                require(rFerns)\n",
    "                require(Boruta)\n",
    "                labels_vector = as.factor(labels_vector)\n",
    "                feat_matrix <- Reshape(feat_matrix, feat_x[1])\n",
    "                feat_data = data.frame(feat_matrix)\n",
    "                colnames(feat_data) <- 1:feat_x[2]\n",
    "                feat_data <- feat_data %>%\n",
    "                    mutate('Labels' = labels_vector)\n",
    "                boruta.train <- Boruta(feat_data$Labels ~ ., data= feat_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "                thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "                boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "                    ''')\n",
    "            boruta_signif = robjects.globalenv['boruta_signif']\n",
    "            robjects.r.rm(\"feat_matrix\")\n",
    "            robjects.r.rm(\"labels_vector\")\n",
    "            robjects.r.rm(\"feat_data\")\n",
    "            robjects.r.rm(\"boruta_signif\")\n",
    "            robjects.r.rm(\"thr\")\n",
    "            topx = []\n",
    "            for index in boruta_signif:\n",
    "                t_index=re.sub(\"`\",\"\",index)\n",
    "                topx.append((np.array(feat.values).T)[int(t_index)-1])\n",
    "            topx = np.array(topx)\n",
    "            values = torch.tensor(topx.T, device=device)\n",
    "        elif feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] >= feat.values.shape[1]:\n",
    "            values = feat.values\n",
    "        else:\n",
    "            values = feat.values\n",
    "    \n",
    "    if is_first == 0:\n",
    "        new_x = torch.tensor(values, device=device).float()\n",
    "        is_first = 1\n",
    "    else:\n",
    "        new_x = torch.cat((new_x, torch.tensor(values, device=device).float()), dim=1)\n",
    "    \n",
    "# Node embedding generation using GCN for each input network with hyperparameter tuning   \n",
    "for n in range(len(node_networks)):\n",
    "    netw_base = node_networks[n]\n",
    "    with open(data_path_node + 'edges_' + netw_base + '.pkl', 'rb') as f:\n",
    "        edge_index = pickle.load(f)\n",
    "    best_ValidLoss = np.Inf\n",
    "    learning_rate = 0.001\n",
    "    hid_size = 128\n",
    "    av_valid_losses = list()\n",
    "\n",
    "    for ii in range(xtimes2):\n",
    "        data = Data(x=new_x, edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=device).long(),\n",
    "                    edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=device).float(), y=labels) \n",
    "        X = data.x[train_valid_idx]\n",
    "        y = data.y[train_valid_idx]\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=1)\n",
    "\n",
    "        for train_part, valid_part in rskf.split(X, y):\n",
    "            train_idx = train_valid_idx[train_part]\n",
    "            valid_idx = train_valid_idx[valid_part]\n",
    "            break\n",
    "\n",
    "        train_mask = np.array([i in set(train_idx) for i in range(data.x.shape[0])])\n",
    "        valid_mask = np.array([i in set(valid_idx) for i in range(data.x.shape[0])])\n",
    "        data.valid_mask = torch.tensor(valid_mask, device=device)\n",
    "        data.train_mask = torch.tensor(train_mask, device=device)\n",
    "        test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "        data.test_mask = torch.tensor(test_mask, device=device)\n",
    "\n",
    "        in_size = data.x.shape[1]\n",
    "        out_size = torch.unique(data.y).shape[0]\n",
    "        model = module.GCN(in_size=in_size, hid_size=hid_size, out_size=out_size)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model = model.to(device)\n",
    "        data = data.to(device)\n",
    "        min_valid_loss = np.Inf\n",
    "        patience_count = 0\n",
    "\n",
    "        for epoch in tqdm(range(max_epochs)):\n",
    "            emb = train()\n",
    "            this_valid_loss, emb = validate()\n",
    "\n",
    "            if this_valid_loss < min_valid_loss:\n",
    "                min_valid_loss = this_valid_loss\n",
    "                patience_count = 0\n",
    "                this_emb = emb\n",
    "            else:\n",
    "                patience_count += 1\n",
    "\n",
    "            if epoch >= min_epochs and patience_count >= patience:\n",
    "                break\n",
    "\n",
    "        av_valid_losses.append(min_valid_loss.item())\n",
    "\n",
    "    av_valid_loss = round(statistics.median(av_valid_losses), 3)\n",
    "\n",
    "    if av_valid_loss < best_ValidLoss:\n",
    "        best_ValidLoss = av_valid_loss\n",
    "        best_emb_lr = learning_rate\n",
    "        best_emb_hs = hid_size\n",
    "        selected_emb = this_emb\n",
    "\n",
    "    \n",
    "    emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "    with open(emb_file, 'wb') as f:\n",
    "        pickle.dump(selected_emb, f)\n",
    "        pd.DataFrame(selected_emb.cpu()).to_csv(emb_file[:-4] + '.csv')\n",
    "    \n",
    "start2 = time.time()    \n",
    "print('It took ' + str(round(start2 - start, 1)) + ' seconds for node embedding generation (' + str(len(learning_rates)*len(hid_sizes))+ ' trials for ' + str(len(node_networks)) + ' seperate GCNs).')\n",
    "\n",
    "print('SUPREME is integrating the embeddings..')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855f4eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 0 ['clinical'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.984+-0.002, train weighted-f1 = 0.984+-0.002, train macro-f1 = 0.967+-0.011, test accuracy = 0.812+-0.011, test weighted-f1 = 0.811+-0.011, test macro-f1 = 0.667+-0.008\n",
      "Combination 1 ['cna'] >  selected parameters = {'hidden_layer_sizes': (512, 32)}, train accuracy = 0.975+-0.009, train weighted-f1 = 0.975+-0.01, train macro-f1 = 0.962+-0.059, test accuracy = 0.79+-0.014, test weighted-f1 = 0.788+-0.015, test macro-f1 = 0.647+-0.108\n",
      "Combination 2 ['exp'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.966+-0.014, train weighted-f1 = 0.965+-0.014, train macro-f1 = 0.962+-0.029, test accuracy = 0.804+-0.01, test weighted-f1 = 0.802+-0.009, test macro-f1 = 0.653+-0.093\n",
      "Combination 3 ['coe'] >  selected parameters = {'hidden_layer_sizes': (32, 32)}, train accuracy = 0.966+-0.018, train weighted-f1 = 0.966+-0.019, train macro-f1 = 0.938+-0.062, test accuracy = 0.797+-0.012, test weighted-f1 = 0.796+-0.012, test macro-f1 = 0.668+-0.089\n",
      "Combination 4 ['met'] >  selected parameters = {'hidden_layer_sizes': (128, 32)}, train accuracy = 0.975+-0.042, train weighted-f1 = 0.976+-0.043, train macro-f1 = 0.977+-0.042, test accuracy = 0.797+-0.004, test weighted-f1 = 0.795+-0.005, test macro-f1 = 0.656+-0.097\n",
      "Combination 5 ['mut'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.978+-0.018, train weighted-f1 = 0.978+-0.018, train macro-f1 = 0.978+-0.017, test accuracy = 0.804+-0.011, test weighted-f1 = 0.804+-0.009, test macro-f1 = 0.817+-0.104\n",
      "Combination 6 ['clinical', 'cna'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.977+-0.009, train weighted-f1 = 0.977+-0.009, train macro-f1 = 0.963+-0.01, test accuracy = 0.804+-0.013, test weighted-f1 = 0.803+-0.015, test macro-f1 = 0.661+-0.019\n",
      "Combination 7 ['clinical', 'exp'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.986+-0.004, train weighted-f1 = 0.986+-0.004, train macro-f1 = 0.981+-0.009, test accuracy = 0.815+-0.004, test weighted-f1 = 0.813+-0.004, test macro-f1 = 0.67+-0.008\n",
      "Combination 8 ['clinical', 'coe'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.97+-0.017, train weighted-f1 = 0.97+-0.017, train macro-f1 = 0.973+-0.016, test accuracy = 0.815+-0.017, test weighted-f1 = 0.813+-0.02, test macro-f1 = 0.666+-0.108\n",
      "Combination 9 ['clinical', 'met'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.985+-0.001, train weighted-f1 = 0.986+-0.001, train macro-f1 = 0.972+-0.004, test accuracy = 0.826+-0.021, test weighted-f1 = 0.824+-0.024, test macro-f1 = 0.677+-0.11\n",
      "Combination 10 ['clinical', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512, 32)}, train accuracy = 0.983+-0.002, train weighted-f1 = 0.983+-0.002, train macro-f1 = 0.964+-0.023, test accuracy = 0.804+-0.006, test weighted-f1 = 0.801+-0.006, test macro-f1 = 0.664+-0.088\n",
      "Combination 11 ['cna', 'exp'] >  selected parameters = {'hidden_layer_sizes': (16,)}, train accuracy = 0.902+-0.045, train weighted-f1 = 0.902+-0.045, train macro-f1 = 0.875+-0.056, test accuracy = 0.786+-0.024, test weighted-f1 = 0.784+-0.021, test macro-f1 = 0.638+-0.021\n",
      "Combination 12 ['cna', 'coe'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.945+-0.025, train weighted-f1 = 0.944+-0.025, train macro-f1 = 0.932+-0.023, test accuracy = 0.801+-0.031, test weighted-f1 = 0.799+-0.029, test macro-f1 = 0.792+-0.098\n",
      "Combination 13 ['cna', 'met'] >  selected parameters = {'hidden_layer_sizes': (128, 32)}, train accuracy = 0.975+-0.034, train weighted-f1 = 0.975+-0.034, train macro-f1 = 0.961+-0.045, test accuracy = 0.775+-0.006, test weighted-f1 = 0.772+-0.004, test macro-f1 = 0.636+-0.089\n",
      "Combination 14 ['cna', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.98+-0.038, train weighted-f1 = 0.98+-0.037, train macro-f1 = 0.941+-0.036, test accuracy = 0.801+-0.024, test weighted-f1 = 0.8+-0.023, test macro-f1 = 0.779+-0.088\n",
      "Combination 15 ['exp', 'coe'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.982+-0.004, train weighted-f1 = 0.982+-0.004, train macro-f1 = 0.978+-0.009, test accuracy = 0.808+-0.013, test weighted-f1 = 0.808+-0.013, test macro-f1 = 0.8+-0.089\n",
      "Combination 16 ['exp', 'met'] >  selected parameters = {'hidden_layer_sizes': (512, 32)}, train accuracy = 0.98+-0.006, train weighted-f1 = 0.98+-0.006, train macro-f1 = 0.977+-0.01, test accuracy = 0.79+-0.019, test weighted-f1 = 0.787+-0.02, test macro-f1 = 0.802+-0.107\n",
      "Combination 17 ['exp', 'mut'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.981+-0.002, train weighted-f1 = 0.981+-0.002, train macro-f1 = 0.982+-0.01, test accuracy = 0.815+-0.015, test weighted-f1 = 0.815+-0.015, test macro-f1 = 0.818+-0.076\n",
      "Combination 18 ['coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (16,)}, train accuracy = 0.917+-0.01, train weighted-f1 = 0.916+-0.011, train macro-f1 = 0.891+-0.058, test accuracy = 0.772+-0.009, test weighted-f1 = 0.771+-0.01, test macro-f1 = 0.785+-0.09\n",
      "Combination 19 ['coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.955+-0.006, train weighted-f1 = 0.955+-0.005, train macro-f1 = 0.951+-0.02, test accuracy = 0.797+-0.011, test weighted-f1 = 0.79+-0.009, test macro-f1 = 0.657+-0.086\n",
      "Combination 20 ['met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.979+-0.042, train weighted-f1 = 0.979+-0.042, train macro-f1 = 0.965+-0.034, test accuracy = 0.804+-0.028, test weighted-f1 = 0.801+-0.026, test macro-f1 = 0.776+-0.092\n",
      "Combination 21 ['clinical', 'cna', 'exp'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.983+-0.004, train weighted-f1 = 0.983+-0.004, train macro-f1 = 0.984+-0.003, test accuracy = 0.822+-0.009, test weighted-f1 = 0.824+-0.008, test macro-f1 = 0.83+-0.091\n",
      "Combination 22 ['clinical', 'cna', 'coe'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.969+-0.021, train weighted-f1 = 0.969+-0.021, train macro-f1 = 0.955+-0.016, test accuracy = 0.815+-0.028, test weighted-f1 = 0.811+-0.028, test macro-f1 = 0.658+-0.026\n",
      "Combination 23 ['clinical', 'cna', 'met'] >  selected parameters = {'hidden_layer_sizes': (256, 32)}, train accuracy = 0.938+-0.03, train weighted-f1 = 0.938+-0.03, train macro-f1 = 0.945+-0.025, test accuracy = 0.779+-0.01, test weighted-f1 = 0.777+-0.009, test macro-f1 = 0.635+-0.009\n",
      "Combination 24 ['clinical', 'cna', 'mut'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.98+-0.019, train weighted-f1 = 0.98+-0.019, train macro-f1 = 0.949+-0.035, test accuracy = 0.819+-0.014, test weighted-f1 = 0.816+-0.013, test macro-f1 = 0.671+-0.012\n",
      "Combination 25 ['clinical', 'exp', 'coe'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.91+-0.065, train weighted-f1 = 0.909+-0.065, train macro-f1 = 0.873+-0.095, test accuracy = 0.764+-0.036, test weighted-f1 = 0.76+-0.037, test macro-f1 = 0.615+-0.034\n",
      "Combination 26 ['clinical', 'exp', 'met'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.982+-0.044, train weighted-f1 = 0.982+-0.044, train macro-f1 = 0.968+-0.044, test accuracy = 0.797+-0.031, test weighted-f1 = 0.793+-0.032, test macro-f1 = 0.642+-0.024\n",
      "Combination 27 ['clinical', 'exp', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512, 32)}, train accuracy = 0.977+-0.016, train weighted-f1 = 0.977+-0.018, train macro-f1 = 0.936+-0.098, test accuracy = 0.804+-0.006, test weighted-f1 = 0.801+-0.006, test macro-f1 = 0.657+-0.007\n",
      "Combination 28 ['clinical', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.974+-0.015, train weighted-f1 = 0.974+-0.015, train macro-f1 = 0.959+-0.025, test accuracy = 0.808+-0.018, test weighted-f1 = 0.808+-0.018, test macro-f1 = 0.667+-0.098\n",
      "Combination 29 ['clinical', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.981+-0.001, train weighted-f1 = 0.981+-0.001, train macro-f1 = 0.965+-0.011, test accuracy = 0.826+-0.014, test weighted-f1 = 0.823+-0.014, test macro-f1 = 0.664+-0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 30 ['clinical', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.979+-0.012, train weighted-f1 = 0.979+-0.012, train macro-f1 = 0.964+-0.031, test accuracy = 0.819+-0.015, test weighted-f1 = 0.815+-0.013, test macro-f1 = 0.658+-0.005\n",
      "Combination 31 ['cna', 'exp', 'coe'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.974+-0.01, train weighted-f1 = 0.974+-0.01, train macro-f1 = 0.958+-0.022, test accuracy = 0.797+-0.004, test weighted-f1 = 0.796+-0.001, test macro-f1 = 0.741+-0.079\n",
      "Combination 32 ['cna', 'exp', 'met'] >  selected parameters = {'hidden_layer_sizes': (16,)}, train accuracy = 0.917+-0.036, train weighted-f1 = 0.918+-0.036, train macro-f1 = 0.86+-0.062, test accuracy = 0.801+-0.018, test weighted-f1 = 0.801+-0.019, test macro-f1 = 0.659+-0.074\n",
      "Combination 33 ['cna', 'exp', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.974+-0.051, train weighted-f1 = 0.974+-0.05, train macro-f1 = 0.96+-0.105, test accuracy = 0.797+-0.018, test weighted-f1 = 0.797+-0.017, test macro-f1 = 0.662+-0.064\n",
      "Combination 34 ['cna', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.957+-0.027, train weighted-f1 = 0.957+-0.027, train macro-f1 = 0.95+-0.042, test accuracy = 0.772+-0.011, test weighted-f1 = 0.772+-0.013, test macro-f1 = 0.787+-0.087\n",
      "Combination 35 ['cna', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (256, 32)}, train accuracy = 0.961+-0.049, train weighted-f1 = 0.961+-0.051, train macro-f1 = 0.946+-0.084, test accuracy = 0.801+-0.022, test weighted-f1 = 0.797+-0.023, test macro-f1 = 0.655+-0.113\n",
      "Combination 36 ['cna', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (16,)}, train accuracy = 0.968+-0.021, train weighted-f1 = 0.968+-0.021, train macro-f1 = 0.953+-0.037, test accuracy = 0.786+-0.013, test weighted-f1 = 0.784+-0.015, test macro-f1 = 0.781+-0.093\n",
      "Combination 37 ['exp', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (64, 32)}, train accuracy = 0.961+-0.018, train weighted-f1 = 0.961+-0.018, train macro-f1 = 0.966+-0.025, test accuracy = 0.79+-0.015, test weighted-f1 = 0.79+-0.014, test macro-f1 = 0.646+-0.009\n",
      "Combination 38 ['exp', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.97+-0.079, train weighted-f1 = 0.97+-0.078, train macro-f1 = 0.971+-0.108, test accuracy = 0.793+-0.029, test weighted-f1 = 0.793+-0.027, test macro-f1 = 0.819+-0.123\n",
      "Combination 39 ['exp', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.983+-0.001, train weighted-f1 = 0.983+-0.001, train macro-f1 = 0.964+-0.021, test accuracy = 0.808+-0.002, test weighted-f1 = 0.807+-0.004, test macro-f1 = 0.818+-0.096\n",
      "Combination 40 ['coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.986+-0.004, train weighted-f1 = 0.986+-0.004, train macro-f1 = 0.981+-0.008, test accuracy = 0.808+-0.009, test weighted-f1 = 0.805+-0.01, test macro-f1 = 0.654+-0.099\n",
      "Combination 41 ['clinical', 'cna', 'exp', 'coe'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.974+-0.038, train weighted-f1 = 0.974+-0.038, train macro-f1 = 0.978+-0.059, test accuracy = 0.786+-0.017, test weighted-f1 = 0.785+-0.015, test macro-f1 = 0.638+-0.014\n",
      "Combination 42 ['clinical', 'cna', 'exp', 'met'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.964+-0.005, train weighted-f1 = 0.964+-0.005, train macro-f1 = 0.949+-0.015, test accuracy = 0.804+-0.012, test weighted-f1 = 0.803+-0.014, test macro-f1 = 0.661+-0.109\n",
      "Combination 43 ['clinical', 'cna', 'exp', 'mut'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.977+-0.015, train weighted-f1 = 0.977+-0.015, train macro-f1 = 0.981+-0.014, test accuracy = 0.822+-0.006, test weighted-f1 = 0.819+-0.006, test macro-f1 = 0.83+-0.098\n",
      "Combination 44 ['clinical', 'cna', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (256, 32)}, train accuracy = 0.979+-0.003, train weighted-f1 = 0.979+-0.003, train macro-f1 = 0.964+-0.002, test accuracy = 0.79+-0.009, test weighted-f1 = 0.788+-0.01, test macro-f1 = 0.645+-0.008\n",
      "Combination 45 ['clinical', 'cna', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.892+-0.053, train weighted-f1 = 0.892+-0.054, train macro-f1 = 0.733+-0.145, test accuracy = 0.779+-0.026, test weighted-f1 = 0.784+-0.024, test macro-f1 = 0.634+-0.022\n",
      "Combination 46 ['clinical', 'cna', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.981+-0.002, train weighted-f1 = 0.981+-0.002, train macro-f1 = 0.982+-0.003, test accuracy = 0.812+-0.018, test weighted-f1 = 0.81+-0.018, test macro-f1 = 0.657+-0.112\n",
      "Combination 47 ['clinical', 'exp', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (512, 32)}, train accuracy = 0.958+-0.011, train weighted-f1 = 0.958+-0.011, train macro-f1 = 0.943+-0.038, test accuracy = 0.804+-0.015, test weighted-f1 = 0.801+-0.014, test macro-f1 = 0.658+-0.007\n",
      "Combination 48 ['clinical', 'exp', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.936+-0.055, train weighted-f1 = 0.936+-0.055, train macro-f1 = 0.944+-0.077, test accuracy = 0.793+-0.008, test weighted-f1 = 0.792+-0.007, test macro-f1 = 0.651+-0.088\n",
      "Combination 49 ['clinical', 'exp', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.986+-0.002, train weighted-f1 = 0.986+-0.002, train macro-f1 = 0.988+-0.013, test accuracy = 0.822+-0.008, test weighted-f1 = 0.822+-0.008, test macro-f1 = 0.827+-0.095\n",
      "Combination 50 ['clinical', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.972+-0.083, train weighted-f1 = 0.972+-0.084, train macro-f1 = 0.947+-0.07, test accuracy = 0.808+-0.033, test weighted-f1 = 0.806+-0.033, test macro-f1 = 0.658+-0.026\n",
      "Combination 51 ['cna', 'exp', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (256,)}, train accuracy = 0.975+-0.1, train weighted-f1 = 0.975+-0.102, train macro-f1 = 0.939+-0.17, test accuracy = 0.801+-0.047, test weighted-f1 = 0.802+-0.053, test macro-f1 = 0.811+-0.137\n",
      "Combination 52 ['cna', 'exp', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.976+-0.03, train weighted-f1 = 0.976+-0.031, train macro-f1 = 0.947+-0.035, test accuracy = 0.808+-0.031, test weighted-f1 = 0.806+-0.029, test macro-f1 = 0.794+-0.014\n",
      "Combination 53 ['cna', 'exp', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.981+-0.117, train weighted-f1 = 0.981+-0.119, train macro-f1 = 0.982+-0.203, test accuracy = 0.808+-0.047, test weighted-f1 = 0.807+-0.048, test macro-f1 = 0.824+-0.141\n",
      "Combination 54 ['cna', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.984+-0.022, train weighted-f1 = 0.984+-0.022, train macro-f1 = 0.986+-0.04, test accuracy = 0.797+-0.015, test weighted-f1 = 0.794+-0.013, test macro-f1 = 0.808+-0.006\n",
      "Combination 55 ['exp', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.98+-0.01, train weighted-f1 = 0.98+-0.011, train macro-f1 = 0.981+-0.018, test accuracy = 0.797+-0.014, test weighted-f1 = 0.797+-0.014, test macro-f1 = 0.809+-0.101\n",
      "Combination 56 ['clinical', 'cna', 'exp', 'coe', 'met'] >  selected parameters = {'hidden_layer_sizes': (64,)}, train accuracy = 0.974+-0.013, train weighted-f1 = 0.974+-0.013, train macro-f1 = 0.974+-0.021, test accuracy = 0.812+-0.024, test weighted-f1 = 0.811+-0.024, test macro-f1 = 0.648+-0.112\n",
      "Combination 57 ['clinical', 'cna', 'exp', 'coe', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32,)}, train accuracy = 0.976+-0.017, train weighted-f1 = 0.976+-0.017, train macro-f1 = 0.961+-0.023, test accuracy = 0.808+-0.01, test weighted-f1 = 0.805+-0.01, test macro-f1 = 0.655+-0.007\n",
      "Combination 58 ['clinical', 'cna', 'exp', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.982+-0.025, train weighted-f1 = 0.982+-0.025, train macro-f1 = 0.966+-0.041, test accuracy = 0.812+-0.023, test weighted-f1 = 0.81+-0.023, test macro-f1 = 0.658+-0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 59 ['clinical', 'cna', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32, 32)}, train accuracy = 0.978+-0.002, train weighted-f1 = 0.978+-0.003, train macro-f1 = 0.961+-0.022, test accuracy = 0.826+-0.004, test weighted-f1 = 0.824+-0.003, test macro-f1 = 0.673+-0.004\n",
      "Combination 60 ['clinical', 'exp', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (128,)}, train accuracy = 0.985+-0.005, train weighted-f1 = 0.985+-0.005, train macro-f1 = 0.987+-0.005, test accuracy = 0.815+-0.006, test weighted-f1 = 0.815+-0.006, test macro-f1 = 0.667+-0.006\n",
      "Combination 61 ['cna', 'exp', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (32, 32)}, train accuracy = 0.921+-0.002, train weighted-f1 = 0.92+-0.003, train macro-f1 = 0.854+-0.06, test accuracy = 0.79+-0.018, test weighted-f1 = 0.788+-0.017, test macro-f1 = 0.653+-0.094\n",
      "Combination 62 ['clinical', 'cna', 'exp', 'coe', 'met', 'mut'] >  selected parameters = {'hidden_layer_sizes': (512,)}, train accuracy = 0.983+-0.005, train weighted-f1 = 0.983+-0.005, train macro-f1 = 0.982+-0.004, test accuracy = 0.815+-0.009, test weighted-f1 = 0.814+-0.011, test macro-f1 = 0.669+-0.017\n",
      "It took 3244.4 seconds in total.\n",
      "SUPREME is done.\n"
     ]
    }
   ],
   "source": [
    "# Running Machine Learning for each possible combination of input network\n",
    "# Input for Machine Learning algorithm is the concatanation of node embeddings (specific to each combination) and node features (if node feature integration is True)    \n",
    "addFeatures = []\n",
    "t = range(len(node_networks))\n",
    "trial_combs = []\n",
    "for r in range(1, len(t) + 1):\n",
    "    trial_combs.extend([list(x) for x in itertools.combinations(t, r)])\n",
    "\n",
    "for trials in range(len(trial_combs)):\n",
    "    node_networks2 = [node_networks[i] for i in trial_combs[trials]]\n",
    "    netw_base = node_networks2[0]\n",
    "    emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "    with open(emb_file, 'rb') as f:\n",
    "        emb = pickle.load(f)\n",
    "\n",
    "    if len(node_networks2) > 1:\n",
    "        for netw_base in node_networks2[1:]:\n",
    "            emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "            with open(emb_file, 'rb') as f:\n",
    "                cur_emb = pickle.load(f)\n",
    "            emb = torch.cat((emb, cur_emb), dim=1)\n",
    "            \n",
    "    if addRawFeat == True:\n",
    "        is_first = 0\n",
    "        addFeatures = feature_networks_integration\n",
    "        for netw in addFeatures:\n",
    "            file = base_path + 'data/' + dataset_name +'/'+ netw +'.pkl'\n",
    "            with open(file, 'rb') as f:\n",
    "                feat = pickle.load(f)\n",
    "            if is_first == 0:\n",
    "                allx = torch.tensor(feat.values, device=device).float()\n",
    "                is_first = 1\n",
    "            else:\n",
    "                allx = torch.cat((allx, torch.tensor(feat.values, device=device).float()), dim=1)   \n",
    "        \n",
    "        if optional_feat_selection == True:     \n",
    "            allx_flat = [item for sublist in allx.tolist() for item in sublist]\n",
    "            allx_temp = robjects.FloatVector(allx_flat)\n",
    "            robjects.globalenv['allx_matrix'] = robjects.r('matrix')(allx_temp)\n",
    "            robjects.globalenv['allx_x'] = robjects.IntVector(allx.shape)\n",
    "            robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "            robjects.globalenv['top'] = boruta_top_features\n",
    "            robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "            robjects.r('''\n",
    "                require(rFerns)\n",
    "                require(Boruta)\n",
    "                labels_vector = as.factor(labels_vector)\n",
    "                allx_matrix <- Reshape(allx_matrix, allx_x[1])\n",
    "                allx_data = data.frame(allx_matrix)\n",
    "                colnames(allx_data) <- 1:allx_x[2]\n",
    "                allx_data <- allx_data %>%\n",
    "                    mutate('Labels' = labels_vector)\n",
    "                boruta.train <- Boruta(allx_data$Labels ~ ., data= allx_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "                thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "                boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "                    ''')\n",
    "            boruta_signif = robjects.globalenv['boruta_signif']\n",
    "            robjects.r.rm(\"allx_matrix\")\n",
    "            robjects.r.rm(\"labels_vector\")\n",
    "            robjects.r.rm(\"allx_data\")\n",
    "            robjects.r.rm(\"boruta_signif\")\n",
    "            robjects.r.rm(\"thr\")\n",
    "            topx = []\n",
    "            for index in boruta_signif:\n",
    "                t_index=re.sub(\"`\",\"\",index)\n",
    "                topx.append((np.array(allx).T)[int(t_index)-1])\n",
    "            topx = np.array(topx)\n",
    "            emb = torch.cat((emb, torch.tensor(topx.T, device=device)), dim=1)\n",
    "            print('Top ' + str(boruta_top_features) + \" features have been selected.\")\n",
    "        else:\n",
    "            emb = torch.cat((emb, allx), dim=1)\n",
    "    \n",
    "    data = Data(x=emb, y=labels)\n",
    "    train_mask = np.array([i in set(train_valid_idx) for i in range(data.x.shape[0])])\n",
    "    data.train_mask = torch.tensor(train_mask, device=device)\n",
    "    test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "    data.test_mask = torch.tensor(test_mask, device=device)\n",
    "    X_train = pd.DataFrame(data.x[data.train_mask].cpu().numpy())\n",
    "    X_test = pd.DataFrame(data.x[data.test_mask].cpu().numpy())\n",
    "    y_train = pd.DataFrame(data.y[data.train_mask].cpu().numpy()).values.ravel()\n",
    "    y_test = pd.DataFrame(data.y[data.test_mask].cpu().numpy()).values.ravel()\n",
    "    \n",
    "    if int_method == 'MLP':\n",
    "        params = {'hidden_layer_sizes': [(16,), (32,),(64,),(128,),(256,),(512,), (32, 32), (64, 32), (128, 32), (256, 32), (512, 32)]}\n",
    "        search = RandomizedSearchCV(estimator = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True), \n",
    "                                    return_train_score = True, scoring = 'f1_macro', \n",
    "                                    param_distributions = params, cv = 4, n_iter = xtimes, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True,\n",
    "                              hidden_layer_sizes = search.best_params_['hidden_layer_sizes'])\n",
    "        \n",
    "    elif int_method == 'XGBoost':\n",
    "        params = {'reg_alpha':range(0,6,1), 'reg_lambda':range(1,5,1),\n",
    "                  'learning_rate':[0, 0.001, 0.01, 1]}\n",
    "        fit_params = {'early_stopping_rounds': 10,\n",
    "                     'eval_metric': 'mlogloss',\n",
    "                     'eval_set': [(X_train, y_train)]}\n",
    "        \n",
    "              \n",
    "        search = RandomizedSearchCV(estimator = XGBClassifier(use_label_encoder=False, n_estimators = 1000, \n",
    "                                                                  fit_params = fit_params, objective=\"multi:softprob\", eval_metric = \"mlogloss\", \n",
    "                                                                  verbosity = 0), return_train_score = True, scoring = 'f1_macro',\n",
    "                                        param_distributions = params, cv = 4, n_iter = xtimes, verbose = 0)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        model = XGBClassifier(use_label_encoder=False, objective=\"multi:softprob\", eval_metric = \"mlogloss\", verbosity = 0,\n",
    "                              n_estimators = 1000, fit_params = fit_params,\n",
    "                              reg_alpha = search.best_params_['reg_alpha'],\n",
    "                              reg_lambda = search.best_params_['reg_lambda'],\n",
    "                              learning_rate = search.best_params_['learning_rate'])\n",
    "                            \n",
    "    elif int_method == 'RF':\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        params = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 100)]}\n",
    "        search = RandomizedSearchCV(estimator = RandomForestClassifier(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4,  n_iter = xtimes, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model=RandomForestClassifier(n_estimators = search.best_params_['n_estimators'])\n",
    "\n",
    "    elif int_method == 'SVM':\n",
    "        params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "        search = RandomizedSearchCV(SVC(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4, n_iter = xtimes, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model=SVC(C = search.best_params_['C'],\n",
    "                  gamma = search.best_params_['gamma'])\n",
    "\n",
    " \n",
    "    av_result_acc = list()\n",
    "    av_result_wf1 = list()\n",
    "    av_result_mf1 = list()\n",
    "    av_tr_result_acc = list()\n",
    "    av_tr_result_wf1 = list()\n",
    "    av_tr_result_mf1 = list()\n",
    " \n",
    "        \n",
    "    for ii in range(xtimes2):\n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        y_pred = [round(value) for value in predictions]\n",
    "        preds = model.predict(pd.DataFrame(data.x.cpu().numpy()))\n",
    "        av_result_acc.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "        av_result_wf1.append(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "        av_result_mf1.append(round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "        tr_predictions = model.predict(X_train)\n",
    "        tr_pred = [round(value) for value in tr_predictions]\n",
    "        av_tr_result_acc.append(round(accuracy_score(y_train, tr_pred), 3))\n",
    "        av_tr_result_wf1.append(round(f1_score(y_train, tr_pred, average='weighted'), 3))\n",
    "        av_tr_result_mf1.append(round(f1_score(y_train, tr_pred, average='macro'), 3))\n",
    "        \n",
    "    if xtimes2 == 1:\n",
    "        av_result_acc.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "        av_result_wf1.append(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "        av_result_mf1.append(round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "        av_tr_result_acc.append(round(accuracy_score(y_train, tr_pred), 3))\n",
    "        av_tr_result_wf1.append(round(f1_score(y_train, tr_pred, average='weighted'), 3))\n",
    "        av_tr_result_mf1.append(round(f1_score(y_train, tr_pred, average='macro'), 3))\n",
    "        \n",
    "\n",
    "    result_acc = str(round(statistics.median(av_result_acc), 3)) + '+-' + str(round(statistics.stdev(av_result_acc), 3))\n",
    "    result_wf1 = str(round(statistics.median(av_result_wf1), 3)) + '+-' + str(round(statistics.stdev(av_result_wf1), 3))\n",
    "    result_mf1 = str(round(statistics.median(av_result_mf1), 3)) + '+-' + str(round(statistics.stdev(av_result_mf1), 3))\n",
    "    tr_result_acc = str(round(statistics.median(av_tr_result_acc), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_acc), 3))\n",
    "    tr_result_wf1 = str(round(statistics.median(av_tr_result_wf1), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_wf1), 3))\n",
    "    tr_result_mf1 = str(round(statistics.median(av_tr_result_mf1), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_mf1), 3))\n",
    "    \n",
    "    print('Combination ' + str(trials) + ' ' + str(node_networks2) + ' >  selected parameters = ' + str(search.best_params_) + \n",
    "      ', train accuracy = ' + str(tr_result_acc) + ', train weighted-f1 = ' + str(tr_result_wf1) +\n",
    "      ', train macro-f1 = ' +str(tr_result_mf1) + ', test accuracy = ' + str(result_acc) + \n",
    "      ', test weighted-f1 = ' + str(result_wf1) +', test macro-f1 = ' +str(result_mf1))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('It took ' + str(round(end - start, 1)) + ' seconds in total.')\n",
    "print('SUPREME is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65aa8331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1292, 1294, 1193, ...,  442,  321,  831]),\n",
       " array([ 817, 1049,  777,   17,  413,  836, 1140,  333, 1340,  894, 1094,\n",
       "         217,  395,  904,  526,  561,  409,   69,  111, 1246, 1310, 1282,\n",
       "         986,  696,  982,  547,   82,   18,  331,  211, 1335,  198,  197,\n",
       "         370,  866, 1319,  403,  807,  337,   54,  458, 1251,  615,  186,\n",
       "         858, 1204,  766, 1287,  162,  169,  202, 1304,  150,  312,  737,\n",
       "         572,  677, 1007, 1168,  417,  775,  954,    6,  221,  160,  709,\n",
       "         511,  863,  228,  580, 1347,  518, 1181,  489,  189, 1067,  440,\n",
       "         854,  153,  405,   36,  471,  231,  539,  657,  268, 1136,  330,\n",
       "         159,  315,  987, 1265,  711, 1029, 1351, 1207,  372, 1075, 1232,\n",
       "          70,  924,  497,  896,  180, 1103,  885,  276, 1215,  181,  123,\n",
       "         456, 1368, 1010, 1020,  769,  569, 1260,  250, 1255, 1302,  755,\n",
       "         812,  675,  998,  981,  126,  512,  488,  394,  869,  452,  946,\n",
       "        1150,  158,  151, 1356,   66,  277,  260, 1338,  963,  648,  459,\n",
       "         887, 1026, 1183,   58,  216,  509,  389, 1228,  575, 1170,  426,\n",
       "         519,  455, 1337,  232, 1073,  446,  280,  453, 1273,  484,  843,\n",
       "        1046,  907, 1225,  113,  919,  604,  613,  504,  171,  974,  827,\n",
       "        1296,  933,  475, 1070,  573,  144,  247, 1259,   46,  989,  338,\n",
       "          14, 1040, 1202,  779,  991,  398,  849, 1042,  999, 1297,  266,\n",
       "         348,  161,  824, 1218,  392, 1146, 1050,  121,  533,   83, 1053,\n",
       "        1367,  443,  251,  174,  821, 1119,  245,  712, 1035,  922,  743,\n",
       "        1061,  951,    8,  925,  555,  421,  418,  667,  438,  809,  643,\n",
       "          26, 1157,  808,  811, 1248,  179, 1317,   64,   30,   25,  467,\n",
       "         485,  184,  470,  591, 1352, 1032,  477,  451,  132,  199,  474,\n",
       "        1177,  408,  410,  112,  283, 1303,   96,  942,  995,  678,  522,\n",
       "         813,  669,  110,  724,   42, 1018, 1039,  614, 1062,  503, 1133,\n",
       "         961]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260551d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "if not os.path.exists(file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump((train_valid_idx, test_idx ),f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bf5c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use pre-defined split\n"
     ]
    }
   ],
   "source": [
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "if os.path.exists(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        new_train_valid_idx, new_test_idx = pickle.load(f)\n",
    "    print('use pre-defined split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f84cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1292, 1294, 1193, ...,  442,  321,  831])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c24861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
